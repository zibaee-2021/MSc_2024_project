MSc student project 
-
---
#### SHELL SCRIPT LAUNCHERS:

You can run the Python scripts via shell scripts in `launchers` directory in the top level of the project folder:
`sh launchers/tokenise_embed_train.sh` which calls `src/diffusion/tokenise_embed_train.py` to run all scripts.  
Alternatively, for demonstration, each part can be run separately:  
- `sh launchers/_1_run_tokeniser.sh`  
- `sh launchers/_2_build_embeddings.sh` 
- `sh launchers/load_datasets`
- `sh launchers/_3_train_model.sh`

> Note: running a launcher shell script will automatically activate the conda env (`diffSock` on `joe-desktop`).
> If you try to manually activate the conda env in the terminal before running a shell script, it seems to cause 
> some form of conflict and fails to execute the scripts, throwing error messages about being unable to find required 
> libraries (like NumPy and Pandas).

---
#### TOKENISER

`src/preprocessing_funcs/tokeniser.py`

(The `tokenise_embed_train.py` script is currently set up to first empty the `mmCIF` and `tokenised` 
directories, to remove all pre-downloaded `mmCIF` files and pre-tokenised `.ssv` files, so that they downloaded/generated 
from scratch. If you don't want this to happen but instead to re-use the files in those directories, then you must 
comment these lines out manually on lines 11 & 12. Conversely the `_1_run_tokeniser.sh` script will call `tokeniser.py`
directly via its `if __name__ == __main__` wherein it is currently set up to not empty the 2 directories. So this time
you would need to uncomment them on lines 616 & 619).

Specifying the `PDBids`/`PDBid_chains` to use for training the model, via `tokeniser.py`.  
Before doing anything, you must decide which proteins will be used for training the model. 
`tokeniser.py` builds a list of `PDBids` and/or `PDBid_chains` according to paths passed to it, all of these are 
optional. Without them, defaults will be used.
1. User-specified directory path from where pre-downloaded `mmCIF` files will be read.  
The default is `mmCIF` directory at `src/diffusion/diff_data`. Whichever `.cif` files are found there will be parsed and 
tokenised (unless it finds corresponding `.ssv` files already in `tokenised` directory at `src/diffusion/diff_data`, 
indicating which, if any, have already been tokenised). 
2. User-specified file path to any existing `.lst` file of `PDBids` and/or `PDBid_chains`.   
A number of these `.lst` files is located in `PDBid_list` at `src/diffusion/diff_data`.   
The default is `None`.
3. User-specified string of `PDBid` and/or `PDBid_chain`; or a Python list of `PDBid` and/or `PDBid_chain` strings.    
(Either a single string or Python list of strings can be passed to `pdb_ids` argument.)
The default is `None`.  

If by the end of this, the list of `PDBids` is still empty, this will be reported (in print statement) and is hard-coded 
to use `src/diffusion/diff_data/PDBid_list/pdbchains_565.lst` (for simpler testing/demo experience).

---
#### PROTEIN LANGUAGE MODEL EMBEDDER

`src/pLM/plm_embedder.py`  

(The `tokenise_embed_train.py` script is currently set up to first empty the `emb`
directory, to remove all pre-downloaded `mmCIF` files and pre-tokenised `.ssv` files, so that they downloaded/generated 
from scratch. If you don't want this to happen but instead to re-use the files in those directories, then you must 
comment these lines out manually on line 16. Conversely the `_2_build_embeddings.sh` script will call `plm_embedder.py`
directly via its `if __name__ == __main__` wherein it is currently set up to not empty that directory. So this time
you would need to uncomment it on line 100).

The choice of which `PDBid_chains` to build protein language model (PLM) embeddings for is implicitly indicated by the tokeniser.  
`tokeniser.py` (which must be run *before* the `plm_embedder.py`) writes tokenised data to `.ssv` files in `src/diffusion/diff_data/tokenised`.  
`plm_embedder.py` reads these `.ssv` files in and extracts the `PDBid_chain` and amino acid sequence directly from each one.  
(In this way, it is guaranteed that in the subsequent section, the neural networks which train on atomic-level data 
are aligned with those that train on amino acid sequence-level data.)  

---
#### MODEL TRAINING

`src/diffusion/pytorch_protfold_allatomclustrain_singlegpu.py` 
`src/diffusion/nndef_protfold_atompyt2.py`

As with the pLM embedder, the choice of which `PDBid_chains` to train on is implicitly indicated by the tokeniser.  
`tokeniser.py` (which must be run *before* the training script) writes tokenised data to `.ssv` files in 
`src/diffusion/diff_data/tokenised`.  
`pytorch_protfold_allatomclustrain_singlegpu.py` reads these `.ssv` files in and prepares two datasets via 
`src/diffusion/dataset_loader.py`. 
It is also necessary that pLM embeddings, generated by `plm_embedder.py`, will already have been run *before* starting
the training script.  
Without these embeddings, saved to `.pt` files in `src/diffusion/diff_data/emb`, the training script will fail to execute.

---
#### ENVIRONMENT, DEPENDENCIES & HARDWARE:

All code was run in miniconda3 environment.

My installed packages on MacOS 14.7.2 Intel (CPU is suitable for tokenisation):
- Conda 24.11.2 (miniconda3)
- Python 3.12.3
- NumPy 1.26.4
- Pandas 2.2.3
- Biopython 1.84
- Requests 2.32.3
- Yaml 0.2.5
- Matplotlib 3.10.0

Installed packages on Rocky Linux ... 
- Conda 24.11.2 (miniconda3)
- Python 3.12.3
- NumPy 2.2... 
- Pandas 2.2.3
- Biopython 1.84
- Requests 2.32.3
- Yaml 0.2.5
- Matplotlib 3.10.0

- PyTorch 2.3... 
- torchvision 0.18.1
- transformers 4.46.2 (HuggingFace package)

---
##### Summary of Python scripts and other files:

- `src/preprocessing_funcs` contains 6 Python scripts and 2 text files that are purely documentation.
  - The following 4 scripts are needed for generating a suitable list of proteins and tokenising their `mmCIF` data.
    - `single_dom_builder.py` builds a dataset of single domain protein using CATH, and downloads their `mmCIF` files.
    - `tokeniser.py` tokenises residues of protein sequence & structural data of corresponding atoms of 1 protein chain, per PDB id from given list of ids. 
    - `cif_parser.py` parses `mmCIF` files in preparation for tokenisation by `tokeniser.py`.
    - `api_caller.py` HTTP client to download cif files from `files.rcsb.org` or FASTA sequences from `www.uniprot.org`.
  
  - The following 2 scripts are "helper functions", but not currently used:
    - `faa_to_atoms.py` helper function to convert a given amino acid sequence to their corresponding atomic sequence.
    - `FASTA_reader.py` helper function to read amino acid sequence of protein given its Uniprot Accession number.

To run `single_domain_builder.py`, the `if __name__ == '__main__':` is currently set up to run the main function 
`parse_single_dom_prots_and_write_csv()` which reads the locally downloaded `cath-domain-list.txt` 
(which is excluded from git tracking due to size, but should be found on local machine in 
`data/dataset/big_files_to_git_ignore/CATH`.) This function saves the parsed data for a selection of the 
single-domain proteins in which a unique structural and functional category is represented only once and which satisfy
certain constraints on the structural data - resulting in a list of only 573 single domain proteins.   
Hence if you want to see this run, simply use `python3 single_dom_builder.py`.

----
##### Layout of functions in src/preprocessing_funcs, src/pLM & data_layer:

The format these scripts follows is for there to be 1 to 3 public functions in one script. 
The public functions are positioned from the bottom up of the script and in the order in which they are called. 
Each public function may call private functions (indicated by underscore prefix) and these are also positioned from 
bottom up in the order in which they are called.

For example: 
```python
def public_func2(val1):
    return some_operation / val1

def _private_func2(res)
    return some_operation * res
    
def _private_func1()
    return some_operation

def public_func1():
    result = _private_func1()
    return _private_func2(result)

if __name__ == '__main__':
    value1 = public_func1()
    public_func2(value1)
```

----
#### COMPILING LIST OF SINGLE-DOMAIN PROTEINS VIA `CATH` WEBSERVER:

Performed by `single_dom_builder.py`

The entire list of domain ids from the latest official version from the file `cath-domain-list.txt` was manually downloaded from CATH webserver at:
`http://download.cathdb.info/cath/releases/all-releases/latest_release/cath-classification-data/cath-domain-list.txt` 
and saved to `data/dataset/big_files_to_git_ignore/CATH/cath-domain-list.txt` (which I haven't added to git but is on `joe-desktop`).
Starting then from 500,238 proteins (and 12 columns of data fields), the dataset is filter by process of elimination:
- removing records for NMR structures, obsolete structures & low resolution X-ray structures
- keep only proteins of classes 1, 2 or 3
- remove data fields that are related to sequence identity 
- store PDBids in new column, which helps filter in those with only one domain
- filter in PDBs with unique 'Architecture', 'Topology' and 'HomologousSF' values.
  - results in reducing number of PDBs from 28,496 proteins to only 573, but this provides a diverse dataset and serves 
   well as a 'dummy' dataset. 
- The `PDB_Id`, `DomainID`, `Architecture` , `Topology`, `HomologousSF`, `Domain_len`, `Angstroms` fields of this 573 
single-domain diverse set of proteins PDBids is saved to  `data/dataset/big_files_to_git_ignore/CATH/SD_573_CIFs.csv`
  (Ref: Orengo et al. 1997 'CATH â€” a hierarchic classification of protein domain structures' Structure 1997, Vol 5 No 8, pp1093-1108)

----
#### Data handling 'helper' functions:

`data_layer/data_handler.py` contains about 30 functions that focus on reading, writing and moving data.
(The original directory structure design was to store all data in the top-level `data` folder. However in the current
format I have most of the data like `mmCIF` files, tokenisation, pLM embeddings in `src/diffusion/diff_data`, which was
done for temporary reasons, but has remained there for now. (I will move them out to the top-level data directory in future).)

---
END